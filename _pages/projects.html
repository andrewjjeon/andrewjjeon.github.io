---
layout: archive
title: "Projects"
permalink: /projects/
author_profile: true
---




<h2>Sensor Fusion for Autonomous Rover Navigation</h2>

<p>
  I am leading the testing and evaluation of a sensor fusion inertial navigation system with 5 sensing modalities on a Rover with a solid-state LiDAR system.
  I tuned navigation system and sensor parameters to achieve an Absolute Trajectory Error of 9.12091m across multi-kilometer trajectories. 

  MINs is a Multisensor-aided Inertial Navigation System that can fuse up to 5 sensing modalities (IMU, wheel encoders, camera, GNSS, and LiDAR) 
  in a Kalman Filter.<a href="https://github.com/rpng/mins" target="_blank">MINs</a> I have tuned MINs on public datasets such as the 
  <a href="https://sites.google.com/view/complex-urban-dataset" target="_blank">KAIST Urban Dataset</a> as well as our labs rover runs.
</p>


<div style="display: flex; justify-content: center; gap: 5px; text-align: center;">
  <div>
    <img src="/images/MINSurban28.gif" alt="MINs on KAIST dataset" style="max-width: 60%; height: auto;">
    <p>MINs on KAIST car dataset</p>
  </div>
  <div>
    <img src="/images/labmins1.gif" alt="MINs on our actual rover" style="max-width: 60%; height: auto;">
    <p>MINs on our actual rover</p>
  </div>
</div>






<h2>Foundation Model Pose Estimation for Robot Grasping</h2>

<p>
  FoundationPose is a Unified foundation model for 6D pose estimation and tracking, 
  it has both CAD model-based configurations and model-free configurations.
  Further details can be found in <a href="https://nvlabs.github.io/FoundationPose/" target="_blank">FoundationPose</a>
</p>

<p>
  I am leading the development of a pipeline that runs multiple instances of the foundation model, processes the transformation and pose matrices to enable precise grasping.
  I am also generating synthetic data, ground truth pose matrices, and robot camera frame visualizations with Pybullet for evaluation.
  The foundationpose object instance and synthetic data to evaluate the robot instance are shown below.
</p>


<div style="display: flex; justify-content: center; gap: 5px; text-align: center;">
  <div>
    <img src="/images/fp_block.JPG" alt="Block Diagram" style="max-width: 60%; height: auto;">
    <p>Block Diagram of Proposed Approach</p>
  </div>
  <div>
    <img src="/images/fp_ketchup.gif" alt="FoundationPose object instance" style="max-width: 60%; height: auto;">
    <p>FoundationPose Object Instance Demo on HOPE Dataset Ketchup</p>
  </div>
</div>


<div style="text-align: center;">
  <div style="display: flex; justify-content: center; gap: 10px;">
    <img src="/images/pose_vis.png" alt="robotgt" style="max-width: 33%; height: auto;">
    <img src="/images/handposeviz.png" alt="handgt" style="max-width: 33%; height: auto;">
    <img src="/images/baseposevis.png" alt="basegt" style="max-width: 33%; height: auto;">
  </div>
  <p>Synthetic Robot Pose Data Generation for Evaluation of FoundationPose Robot Instance</p>
</div>

<br><br>
<br><br>




<h2>3D Semantic Segmentation for Robot Navigation</h2>

<p>
  VLMaps is a spatial map representation that embeds pretrained visual-language features with a 3D reconstruction
  and projects to a top-down 2D map. VLMaps embeds visual features from an LSeg visual encoder to points in a point cloud. 
  These points are then projected to a top-down navigation map where only the point with the highest height is kept. After 
  this, the visual features are compared through cosine similarity to textual features from a LSeg text encoder to determine 
  the semantic label of the point. Due to the top-down projection, the robot wouldn't be capable of 3D navigation such as 
  “go to the plant below the table.” 
  
  Further details can be found in <a href="https://vlmaps.github.io/" target="_blank">VLMaps</a>
</p>

<p>
  To address this limitation, I implemented a voxel grid projection by populating the voxel grid cells with their corresponding 
  points. When multiple points fall in the same voxel cell I average all their embeddings to generate a singular embedding. 
  This resulted in a best class segmentation accuracy of 0.907 and the robot being able to navigation in 3D as opposed to 2D.

</p>

<div style="text-align: center;">
  <img src="/images/3dvlmaps_block.jpg" alt="Block Diagram" style="max-width: 100%; height: auto;">
  <p>Block diagram of 3DVLMaps.</p>
</div>


<div style="text-align: center;">
  <div style="display: flex; justify-content: center; gap: 10px;">
    <img src="/images/segresult.png" alt="House 3D Segmentation Result" style="max-width: 49%; height: auto;">
    <img src="/images/3dvlmaps_key.png" alt="House 3D Segmentation Key" style="max-width: 49%; height: auto;">
  </div>
  <p>3D segmentation demonstration of household scene.</p>
</div>


<div style="text-align: center;">
  <div style="display: flex; justify-content: center; gap: 10px;">
    <img src="/images/realsofa.png" alt="Real Sofa" style="max-width: 49%; height: auto;">
    <img src="/images/seg_sofa.png" alt="Segmented Sofa" style="max-width: 49%; height: auto;">
  </div>
  <p>3D segmentation demonstration of sofa, comparison with real world point cloud</p>
</div>


<div style="text-align: center;">
  <div style="display: flex; justify-content: center; gap: 10px;">
    <img src="/images/table.png" alt="Table" style="max-width: 49%; height: auto;">
    <img src="/images/tableseg.png" alt="Segmented Table" style="max-width: 49%; height: auto;">
  </div>
  <p>3D segmentation demonstration of Table, comparison with real world point cloud</p>
</div>

<p>
  The couch segmentation had an accuracy of 0.827, the table had an accuracy of 0.832
</p>

<br><br>
<br><br>




<h2>Regularization, SVD, and Hyperparameter Tuning to Model Neural Population Dynamics</h2>



<p>
  There is a strong need for techniques that minimize the amount of data needed to learn neural population dynamics 
  due to experimental time and resource constraints. "Active learning of neural population dynamics using two-photon 
  holographic optogenetics" is a NeurIPS 2024 paper that attempts to determine the most effective photostimulation 
  patterns for identifying neural population dynamics. The goal is to select the neurons that have the most informative 
  neural responses to inform a dynamical model of the neural population activity. 
</p>
<p>
  The project’s active learning technique takes advantage of the low-rank structure of the neural population dynamics 
  to determine the most informative photostimulation patterns. It uses SVD to create low rank autoregressive models 
  that predict neural activity. I conducted various regularization and tuning experiments on these models. This resulted 
  in 15-18% improvements (MSE) in model performance. 

  Further details can be found in the paper <a href="https://arxiv.org/abs/2412.02529" 
  target="_blank">Active learning of neural population dynamics</a>
</p>


<div style="text-align: center;">
  <div style="display: flex; justify-content: center; gap: 10px;">
    <img src="/images/fullrank.png" alt="Full Rank 502 neurons model" style="max-width: 49%; height: auto;">
    <img src="/images/lowrank.png" alt="Low Rank 35 neurons model" style="max-width: 49%; height: auto;">
  </div>
  <p>Full Rank Stimulation Responses vs Low Rank Stimulation Responses comparison displays the low rank structure of the 
    neural population dynamics</p>
</div>


<div style="text-align: center;">
  <div style="display: flex; justify-content: center; gap: 10px;">
    <img src="/images/lowrank.png" alt="Low Rank 35 neurons model" style="max-width: 49%; height: auto;">
    <img src="/images/lowrankl2.png" alt="Low Rank 35 neurons model w/ L2 Regularization" style="max-width: 49%; height: auto;">
  </div>
  <p>Low Rank Stimulation Responses vs Low Rank w/ L2 Regularization Stimulation Responses comparison shows the effects of 
    L2 regularization clearly</p>
</div>



<br><br>
<br><br>


<h2>Image Processing for Fisheye Camera Image Object Detection</h2>

<p>
  My team and I participated in the 2024 AI City Challenge Track 4 Fisheye Camera Object Detection Challenge. 
  Fisheye cameras have a wide field of view, which allows them to capture wider scenes, but comes with image distortion
  that can impede computer vision tasks. My main contribution to our team ensemble learning solution was using OpenCV image 
  processing to transform the full training set of colored + nighttime images to all black&white and retraining the YOLO 
  model on the transformed dataset. This resulted in 9% improved mAP object detection performance on night-time images 
  in the test set.
</p>

<p>
  Specifically, the images below powerfully show the improvement in performance my black&white image transformation and 
  subsequent retraining achieved.
</p>


  <div style="text-align: center;">
    <div style="display: flex; justify-content: center; gap: 10px;">
      <img src="/images/before1.jpg" alt="Original Model Detections" style="max-width: 40%; height: auto;">
      <img src="/images/after1.jpg" alt="Transformed Image Trained Model Detections" style="max-width: 40%; height: auto;">
    </div>
    <p>Original model detections vs Transformed Image model detections</p>
  </div>
  
  
  <div style="text-align: center;">
    <div style="display: flex; justify-content: center; gap: 10px;">
      <img src="/images/before2.png" alt="Original Model Detections" style="max-width: 40; height: auto;">
      <img src="/images/after2.png" alt="Transformed Image Trained Model Detections" style="max-width: 40%; height: auto;">
    </div>
    <p>Original model detections vs Transformed Image model detections</p>
  </div>





  <br><br>
  <br><br>
  
  
  <h2>Military Target Classification</h2>

  <p>
    My team and I built a comprehensive object detection pipeline starting from dataset creation/annotation 
    to model evaluation and fine-tuning. We gathered images, annotated and conducted data augmentation
    in roboflow to increase our dataset size to 2,652 images. After this we trained various YOLOv8 models on this data. 
    We then performed hyperparameter tuning to narrow down to the best learning-rate, epochs, optimizer, image size, 
    loss function, and other hyperparameters. Further details can be found in the repo: 
    <a href="https://github.com/Naif-Ganadily/Friend-or-Foe-Multi-Modal-Military-Target-Identification" target="_blank">FFMMMTI</a>
  </p>


  <div style="text-align: center;">
    <img src="/images/ffmmmti.gif" alt="FFMMMTI live demo" style="max-width: 100%; height: auto;">
    <p>Demonstration of Friend or Foe in action on test images</p>
  </div>
  
  <div style="text-align: center;">
    <img src="/images/yolov8.jpg" alt="YOLOv8s (small) validation set detections" style="max-width: 100%; height: auto;">
    <p>YOLOv8s (small) validation set detections</p>
  </div>
  
  <p>YOLOv8 Model Performance on Validation Set</p>

  <table style="border-collapse: collapse; width: 100%; text-align: center;">
    <thead>
      <tr>
        <th style="border: 1px solid black; padding: 8px; background-color: #f2f2f2;">Model</th>
        <th style="border: 1px solid black; padding: 8px; background-color: #f2f2f2;">Dataset</th>
        <th style="border: 1px solid black; padding: 8px; background-color: #f2f2f2;">Hyperparameters</th>
        <th style="border: 1px solid black; padding: 8px; background-color: #f2f2f2;">mAP50</th>
        <th style="border: 1px solid black; padding: 8px; background-color: #f2f2f2;">mAP50-95</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="border: 1px solid black; padding: 8px;">YOLOv8n</td>
        <td style="border: 1px solid black; padding: 8px;">Friend_or_foe_class_consolidation_object/10</td>
        <td style="border: 1px solid black; padding: 8px;">Ideal (50 epoch)</td>
        <td style="border: 1px solid black; padding: 8px;">0.711</td>
        <td style="border: 1px solid black; padding: 8px;">0.536</td>
      </tr>
      <tr>
        <td style="border: 1px solid black; padding: 8px;">YOLOv8s</td>
        <td style="border: 1px solid black; padding: 8px;">Friend_or_foe_class_consolidation_object/7</td>
        <td style="border: 1px solid black; padding: 8px;">Ideal (50 epoch)</td>
        <td style="border: 1px solid black; padding: 8px;">0.736</td>
        <td style="border: 1px solid black; padding: 8px;">0.581</td>
      </tr>
      <tr>
        <td style="border: 1px solid black; padding: 8px;">YOLOv8m</td>
        <td style="border: 1px solid black; padding: 8px;">Friend_or_foe_class_consolidation_object/8</td>
        <td style="border: 1px solid black; padding: 8px;">Ideal (50 epoch)</td>
        <td style="border: 1px solid black; padding: 8px;">0.773</td>
        <td style="border: 1px solid black; padding: 8px;">0.636</td>
      </tr>
      <tr>
        <td style="border: 1px solid black; padding: 8px;">YOLOv8l</td>
        <td style="border: 1px solid black; padding: 8px;">Friend_or_foe_class_consolidation_object/5</td>
        <td style="border: 1px solid black; padding: 8px;">Ideal (25 epoch)</td>
        <td style="border: 1px solid black; padding: 8px;">0.371</td>
        <td style="border: 1px solid black; padding: 8px;">0.307</td>
      </tr>
      <tr>
        <td style="border: 1px solid black; padding: 8px;">YOLOv8x</td>
        <td style="border: 1px solid black; padding: 8px;">Friend_or_foe_class_consolidation_object/9</td>
        <td style="border: 1px solid black; padding: 8px;">Ideal (25 epoch)</td>
        <td style="border: 1px solid black; padding: 8px;">0.757</td>
        <td style="border: 1px solid black; padding: 8px;">0.613</td>
      </tr>
      <tr>
        <td style="border: 1px solid black; padding: 8px;">YOLOv8x-seg</td>
        <td style="border: 1px solid black; padding: 8px;">Friend_or_foe_class_consolidation_object/5</td>
        <td style="border: 1px solid black; padding: 8px;">Ideal (25 epoch)</td>
        <td style="border: 1px solid black; padding: 8px;">0.734</td>
        <td style="border: 1px solid black; padding: 8px;">0.562</td>
      </tr>
    </tbody>
  </table>