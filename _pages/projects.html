---
layout: archive
title: "Projects"
permalink: /projects/
author_profile: true
---



<h2>Novel Object Robotic Grasping with Foundation Model</h2>

<p>
  FoundationPose is a Unified foundation model for 6D pose estimation and tracking, 
  it has both CAD model-based configurations and model-free configurations.
  Further details can be found in <a href="https://nvlabs.github.io/FoundationPose/" target="_blank">FoundationPose</a>
</p>

<p>
  In order to perform novel object grasping with this model, we propose to run 2x instances of FoundationPose. 
  1x on the robot hand or configuration. 1x on the object to grasp. This should give you the respective pose matrix which 
  contains the 3x1 translation(tx, ty, tz) and 3x3 rotation (r11, r12, r13, r21, r22, r23, ...). At this point,
  you should have the <b>robot-to-camera</b> pose matrix and the <b>object-to-camera</b> pose matrix. If you have these
  two matrices, you can now calculate the <b>robot-to-object</b> pose matrix which in turn would enable robotic grasping.
  The more accurate the pose matrix, the more precise the object grasping with your robot hand or configuration. The
  long-term goal is to make this system robust enough to work with whatever robot model and configuration you have i.e. 
  franka robot hand, franka robot hand + link0, PP100 hand + link0 + link1, and so forth. Our rough approach is illustrated below:
</p>


<div style="text-align: center;">
  <img src="/images/fp_block.JPG" alt="Block Diagram" style="max-width: 100%; height: auto;">
  <p>Block diagram of our proposed approach.</p>
</div>

<div style="text-align: center;">
  <img src="/images/fp_ketchup.gif" alt="FoundationPose object instance" style="max-width: 100%; height: auto;">
  <p>Demonstration of FoundationPose object instance estimating pose and tracking a ketchup bottle from the HOPE Dataset.</p>
</div>




<br><br>
<br><br>




<h2>3DVLMaps: 3D Visual-Language Maps for Robot Navigation</h2>

<p>
  VLMaps is a spatial map representation that embeds pretrained visual-language features with a 3D reconstruction
  and projects to a top-down 2D map. VLMaps embeds visual features from an LSeg visual encoder to points in a point cloud. These points are then projected 
  to a top-down navigation map where only the point with the highest height is kept. After this, the visual features are 
  compared through cosine similarity to textual features from a LSeg text encoder to determine the semantic label of the point. 
  Due to the top-down projection, the robot wouldn't be capable of 3D navigation such as “go to the plant below the table.” 
  
  Further details can be found in <a href="https://vlmaps.github.io/" target="_blank">VLMaps</a>
</p>

<p>
  To address this limitation, I implemented a voxel grid projection by populating the voxel grid cells with their corresponding 
  points. When multiple points fall in the same voxel cell I average all their embeddings to generate a singular embedding. 
  This resulted in successful, 3D semantic segmentation of the voxel grid scene, which would allow a robot to navigate in the 
  3D space.
</p>

<div style="text-align: center;">
  <img src="/images/3dvlmaps_block.jpg" alt="Block Diagram" style="max-width: 100%; height: auto;">
  <p>Block diagram of 3DVLMaps.</p>
</div>


<div style="text-align: center;">
  <div style="display: flex; justify-content: center; gap: 10px;">
    <img src="/images/segresult.png" alt="House 3D Segmentation Result" style="max-width: 49%; height: auto;">
    <img src="/images/3dvlmaps_key.png" alt="House 3D Segmentation Key" style="max-width: 49%; height: auto;">
  </div>
  <p>3D segmentation demonstration of household scene.</p>
</div>


<div style="text-align: center;">
  <div style="display: flex; justify-content: center; gap: 10px;">
    <img src="/images/realsofa.png" alt="Real Sofa" style="max-width: 49%; height: auto;">
    <img src="/images/seg_sofa.png" alt="Segmented Sofa" style="max-width: 49%; height: auto;">
  </div>
  <p>3D segmentation demonstration of sofa, comparison with real world point cloud</p>
</div>


<div style="text-align: center;">
  <div style="display: flex; justify-content: center; gap: 10px;">
    <img src="/images/table.png" alt="Table" style="max-width: 49%; height: auto;">
    <img src="/images/tableseg.png" alt="Segmented Table" style="max-width: 49%; height: auto;">
  </div>
  <p>3D segmentation demonstration of Table, comparison with real world point cloud</p>
</div>

<p>
  The couch segmentation had an accuracy of 0.827, the table had an accuracy of 0.832
</p>

<br><br>
<br><br>




<h2>Autoregressive Low Rank Model Regularization and Tuning</h2>



<p>
  There is a strong need for techniques that minimize the amount of data needed to learn neural population dynamics 
  due to experimental time and resource constraints. "Active learning of neural population dynamics using two-photon 
  holographic optogenetics" is a NeurIPS 2024 paper that attempts to determine the most effective photostimulation 
  patterns for identifying neural population dynamics. The goal is to select the neurons that have the most informative 
  neural responses to inform a dynamical model of the neural population activity. 
</p>
<p>
  The project’s active learning technique takes advantage of the low-rank structure of the neural population dynamics 
  to determine the most informative photostimulation patterns. It uses SVD to create low rank autoregressive models 
  that predict neural activity. I conducted various regularization, tuning experiments, closed-form solution implementations, 
  and code refactors on these models. My most significant result was that L2 regularization was more beneficial in full rank 
  models compared to low rank.

  Further details can be found in the paper <a href="https://arxiv.org/abs/2412.02529" 
  target="_blank">Active learning of neural population dynamics</a>
</p>


<div style="text-align: center;">
  <div style="display: flex; justify-content: center; gap: 10px;">
    <img src="/images/fullrank.png" alt="Full Rank 502 neurons model" style="max-width: 49%; height: auto;">
    <img src="/images/lowrank.png" alt="Low Rank 35 neurons model" style="max-width: 49%; height: auto;">
  </div>
  <p>Full Rank Stimulation Responses vs Low Rank Stimulation Responses comparison displays the low rank structure of the 
    neural population dynamics</p>
</div>


<div style="text-align: center;">
  <div style="display: flex; justify-content: center; gap: 10px;">
    <img src="/images/lowrank.png" alt="Low Rank 35 neurons model" style="max-width: 49%; height: auto;">
    <img src="/images/lowrankl2.png" alt="Low Rank 35 neurons model w/ L2 Regularization" style="max-width: 49%; height: auto;">
  </div>
  <p>Low Rank Stimulation Responses vs Low Rank w/ L2 Regularization Stimulation Responses comparison shows the effects of 
    L2 regularization clearly</p>
</div>



<br><br>
<br><br>


<h2>Roadside object detection w/ YOLOv8 in Fisheye Camera Images</h2>

<p>
  My team and I participated in the 2024 AI City Challenge Track 4 Fisheye Camera Object Detection Challenge. 
  Fisheye cameras have a wide field of view, which allows them to capture wider scenes, but comes with image distortion
  that can impede computer vision tasks. My main contribution to our team ensemble learning solution was using OpenCV image 
  processing to transform the full training set of colored + nighttime images to all black&white and retraining the YOLO 
  model on the transformed dataset. This resulted in 5-10% improved mAP object detection performance on night-time images 
  in the test set
</p>

<p>
  Specifically, the images below powerfully show the improvement in performance my black&white image transformation and 
  subsequent retraining achieved. On the left we have detections in a night-time image from the original YOLOv8 model, 
  on the right we have detections in the same night-time image from the YOLOv8 model trained on my full black&white 
  transformed dataset.
</p>


  <div style="text-align: center;">
    <div style="display: flex; justify-content: center; gap: 10px;">
      <img src="/images/before1.jpg" alt="Original Model Detections" style="max-width: 49%; height: auto;">
      <img src="/images/after1.jpg" alt="Transformed Image Trained Model Detections" style="max-width: 49%; height: auto;">
    </div>
    <p>Original model detections vs Transformed Image model detections</p>
  </div>
  
  
  <div style="text-align: center;">
    <div style="display: flex; justify-content: center; gap: 10px;">
      <img src="/images/before2.png" alt="Original Model Detections" style="max-width: 49%; height: auto;">
      <img src="/images/after2.png" alt="Transformed Image Trained Model Detections" style="max-width: 49%; height: auto;">
    </div>
    <p>Original model detections vs Transformed Image model detections</p>
  </div>





  <br><br>
  <br><br>
  
  
  <h2>Friend or Foe: Multi-Modal Military Target Identification (FFMMMTI)</h2>

  <p>
    My team and I built a comprehensive object detection pipeline starting from dataset creation/annotation 
    to model evaluation and fine-tuning. We gathered 1000 images of Russian and USA army, navy, marines, and airforce.
    We also added 100 (null) or irrelevant images to our dataset as well. We annotated and conducted data augmentation
    in roboflow to increase our dataset size to 2,652 images. After this we trained various YOLOv8 models on this data. 
    We then performed hyperparameter tuning to narrow down to the best learning-rate, epochs, optimizer, image size, 
    loss function, and other hyperparameters. Further details can be found in the repo: 
    <a href="https://github.com/Naif-Ganadily/Friend-or-Foe-Multi-Modal-Military-Target-Identification" target="_blank">FFMMMTI</a>
  </p>


  <div style="text-align: center;">
    <img src="/images/ffmmmti.gif" alt="FFMMMTI live demo" style="max-width: 100%; height: auto;">
    <p>Demonstration of Friend or Foe in action on test images</p>
  </div>


  
  <div style="text-align: center;">
    <div style="display: flex; justify-content: center; gap: 10px;">
      <img src="/images/yolov8.jpg" alt="YOLOv8s (small) validation set detections" style="max-width: 49%; height: auto;">
      <img src="/images/ffmmmti_table.png" alt="YOLOv8 versions validation performance" style="max-width: 49%; height: auto;">
    </div>
    <p>System performance</p>
  </div>
  
